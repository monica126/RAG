{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "add6fb06-0ba1-4d20-94d9-7630e0f42972",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langchain_google_genai'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_google_genai\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ChatGoogleGenerativeAI\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_google_genai\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GoogleGenerativeAIEmbeddings\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mprompts\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PromptTemplate\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'langchain_google_genai'"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.vectorstores import Chroma\n",
    "#source: https://gist.github.com/janakiramm/8fcfc6c055c09a6f5dc5248b890f0567 \n",
    "#https://gist.github.com/janakiramm/6546d9734c7872f111b139cda1a8e0de\n",
    "#https://www.youtube.com/watch?v=_GEitI9PU28&ab_channel=JanakiramMSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2bb7880-bef8-4890-8e0e-5f4fb4c85728",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import pyarrow\n",
    "import numpy as np\n",
    "import google.generativeai as genai\n",
    "\n",
    "os.environ['GOOGLE_API_KEY'] = \"AIzaSyDsJmANmxLg_bZuVZTj8BPEPgAd2kJwBhQ\"\n",
    "genai.configure(api_key = os.environ['GOOGLE_API_KEY'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8db83775-d3f3-42a8-a56e-1507a202bee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\")\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98cb91de-7962-47f6-a8b2-a4b91094bf68",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the PDF and create chunks\n",
    "\n",
    "from langchain_community.document_loaders import TextLoader,PyPDFLoader\n",
    "from langchain_text_splitters.character import CharacterTextSplitter\n",
    "loader = PyPDFLoader(\"FewShotText2SQLofLLM.pdf\")\n",
    "#loader = TextLoader(\"FewShotText2SQLofLLM.pdf\")\n",
    "text_documents = loader.load()\n",
    "text_documents\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    separator=\".\",\n",
    "    chunk_size=250,\n",
    "    chunk_overlap=20,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")\n",
    "pages = loader.load_and_split(text_splitter)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9e0f561d-c095-4c21-b972-b4f7eca63135",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Turn the chunks into embeddings and store them in Chroma\n",
    "vectordb=Chroma.from_documents(pages,embeddings)\n",
    "\n",
    "#Configure Chroma as a retriever with top_k=5\n",
    "retriever = vectordb.as_retriever(search_kwargs={\"k\": 5})\n",
    "\n",
    "#Create the retrieval chain\n",
    "template = \"\"\"\n",
    "You are a helpful AI assistant.\n",
    "Answer based on the context provided. \n",
    "context: {context}\n",
    "input: {input}\n",
    "answer:\n",
    "\"\"\"\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "combine_docs_chain = create_stuff_documents_chain(llm, prompt)\n",
    "retrieval_chain = create_retrieval_chain(retriever, combine_docs_chain)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5a8632a1-a5a7-4997-a593-f76203f500b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction-syntax based retrieval is a method that uses the syntactic embeddings of predicted SQL queries to select demonstration examples. It is used to ensure coverage of as many syntax demonstrations as feasible to mitigate potential failures in similarity-based retrieval.\n"
     ]
    }
   ],
   "source": [
    "#Invoke the retrieval chain\n",
    "#response=retrieval_chain.invoke({\"input\":\"What is Hollywood going to start doing?\"})\n",
    "\n",
    "response = retrieval_chain.invoke({\"input\":\"What is prediction-syntax based retrieval?\"})\n",
    "#Print the answer to the question\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "99f5468d-8632-434b-a0f8-3a9d97bce353",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain.chains import StuffDocumentsChain\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "#Initialize Model\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\")\n",
    "\n",
    "#Load the blog\n",
    "loader = WebBaseLoader(\"https://thenewstack.io/the-building-blocks-of-llms-vectors-tokens-and-embeddings/\")\n",
    "docs = loader.load()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d7a819e9-4d66-43b5-9aee-d99143f604b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To understand how LLMs process language, it's essential to grasp the concepts of vectors, tokens, and embeddings. Vectors are numerical representations of data, enabling machines to process text and images. Tokens are the basic units of data, often words or subwords, converted into vectors by a tokenizer. Embeddings are tokens with added semantic context, capturing the meaning and relationships between words, allowing LLMs to understand nuance and perform tasks like sentiment analysis. Tokens are linguistic units, vectors are their mathematical representations, and embeddings are vectors trained to capture semantic relationships, forming the backbone of LLM technology.\n"
     ]
    }
   ],
   "source": [
    "#Define the Summarize Chain\n",
    "template = \"\"\"Write a concise summary of the following:\n",
    "\"{text}\"\n",
    "CONCISE SUMMARY:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "#llm_chain = LLMChain(llm=llm, prompt=prompt)\n",
    "llm_chaim = llm | prompt\n",
    "stuff_chain = StuffDocumentsChain(llm_chain=llm_chain, document_variable_name=\"text\")\n",
    "\n",
    "#Invoke Chain\n",
    "response=stuff_chain.invoke(docs)\n",
    "print(response[\"output_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d877ea50-1aec-4f25-be56-c3a0d4e8bb03",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
